{"nbformat":4,"nbformat_minor":0,"metadata":{"interpreter":{"hash":"64c7587c57fab0328c0f08a8fa64e3e786b56407a0ea30be36fecc0cd89940e6"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"Lab 6 - Logistic Regression.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"ca5amMpOjl4y"},"source":["# Tan Ze Xin Dylan\n","# 1004385"]},{"cell_type":"markdown","metadata":{"id":"d5gMd5H5ja5O"},"source":["## Load the IMDB dataset and create the vocabulary"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hv4Op4-Eja5T","executionInfo":{"status":"ok","timestamp":1635003693359,"user_tz":-480,"elapsed":118180,"user":{"displayName":"Dylan Tan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12191622771912211182"}},"outputId":"94cb1e3d-1e02-45ac-c430-b5df3bbe0059"},"source":["from torchtext.datasets import IMDB\n","from torchtext.data.utils import get_tokenizer\n","from collections import Counter, OrderedDict\n","from torchtext.vocab import vocab\n","\n","EMBEDDING_DIM=50\n","VOCAB_SIZE=20000\n","\n","# Get IMDB dataset\n","imdb = IMDB(split='train')\n","\n","# Load English tokenizer, tagger, parser and NER\n","tokenizer = get_tokenizer('spacy', language='en')\n","\n","# build the vocab\n","counter = Counter()\n","for i, (label, line) in enumerate(imdb):\n","    counter.update(tokenizer(line))\n","\n","ordered_dict = OrderedDict(counter.most_common()[:VOCAB_SIZE])\n","vocab = vocab(ordered_dict)\n","\n","# insert special tokens and set default index to 'unknown'\n","vocab.insert_token('<PAD>', 0)\n","vocab.insert_token('<UNK>', 1)\n","vocab.set_default_index(1)"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:01<00:00, 43.0MB/s]\n"]}]},{"cell_type":"markdown","metadata":{"id":"bNOzRAudja5V"},"source":["## Create embedding vectors from GloVe"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1vcpYTNAja5W","executionInfo":{"status":"ok","timestamp":1635003897403,"user_tz":-480,"elapsed":204062,"user":{"displayName":"Dylan Tan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12191622771912211182"}},"outputId":"0c7ba7f0-1d36-4e2e-99bd-786e0ce97d38"},"source":["import torchtext as text\n","\n","# load glove embeddings\n","vec = text.vocab.GloVe(name='6B', dim=50)\n","# create the embedding matrix, a torch tensor in the shape (num_words+1, embedding_dim)\n","word_emb = vec.get_vecs_by_tokens(vocab.get_itos())"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":[".vector_cache/glove.6B.zip: 862MB [02:40, 5.36MB/s]                           \n","100%|█████████▉| 399999/400000 [00:14<00:00, 27752.50it/s]\n"]}]},{"cell_type":"markdown","metadata":{"id":"byOF1-5vja5X"},"source":["## Build up train/test dataset"]},{"cell_type":"code","metadata":{"id":"-Op4KyyVja5X","executionInfo":{"status":"ok","timestamp":1635003920963,"user_tz":-480,"elapsed":23573,"user":{"displayName":"Dylan Tan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12191622771912211182"}}},"source":["from torch.utils.data import DataLoader\n","from torchtext.data.functional import to_map_style_dataset\n","import torch\n","from torch.utils.data.dataset import random_split\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# transform input text and label to ids\n","def process_text(text):\n","    return vocab(tokenizer(text))\n","\n","label_to_ids = {'pos':0, 'neg':1}\n","\n","# preprocess a batch of raw data (string-like text/labels) into tensors\n","def collate_batch(batch):\n","    label_list, text_list, offsets = [], [], [0]\n","    for (_label, _text) in batch:\n","        label_list.append(label_to_ids[_label])\n","        processed_text = torch.tensor(process_text(_text), dtype=torch.int64)\n","        text_list.append(processed_text)\n","        offsets.append(processed_text.size(0))\n","    # label must be in the same size as target\n","    label_list = torch.tensor(label_list, dtype=torch.float)[:,None]\n","    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n","    text_list = torch.cat(text_list)\n","    return label_list.to(device), text_list.to(device), offsets.to(device)\n","\n","train_iter, test_iter = IMDB()\n","\n","# transform datasets iterator into map style so that they can be repeatedly loaded in a new epoch\n","train_dataset = to_map_style_dataset(train_iter)\n","test_dataset = to_map_style_dataset(test_iter)\n","\n","# Split ratio of 0.95 for train and 0.05 for validation\n","train_size = int(len(train_dataset) * 0.95)\n","split_train_dataset, split_valid_dataset = random_split(train_dataset, [train_size, len(train_dataset) - train_size])\n","\n","train_dataloader = DataLoader(split_train_dataset, batch_size=128,\n","                              shuffle=True, collate_fn=collate_batch)\n","test_dataloader = DataLoader(test_dataset, batch_size=128,\n","                             shuffle=True, collate_fn=collate_batch)\n","# Question 1\n","# Write a validation dataloader by spliting the training data\n","val_dataloader = DataLoader(split_valid_dataset, batch_size=128,\n","                            shuffle=True, collate_fn=collate_batch)"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dzQpl8Tqja5Y"},"source":["## Define the logistic regression model"]},{"cell_type":"code","metadata":{"id":"2vy83ihsja5Z","executionInfo":{"status":"ok","timestamp":1635007789369,"user_tz":-480,"elapsed":294,"user":{"displayName":"Dylan Tan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12191622771912211182"}}},"source":["# logistic model\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class LogisticRegression(nn.Module):\n","    def __init__(self, word_vec, embed_dim):\n","        super().__init__()\n","        # embeddingbag outputs the average of all the words in a sentence\n","        \n","        # Question 2 : Replace the EmbeddingBag using PyTorch builtin functions that does the same job of computing sentence representation by taking average of the word embeddings.\n","        self.embedding = nn.Embedding(*(word_vec.size())).from_pretrained(word_vec, freeze=False)\n","\n","        # Question 3 : Write a Fully Connected Layer (FC layer) with output size of 100 followed by a non-linear activation e.g., ReLU\n","\n","        self.layers = nn.Sequential(\n","            nn.Linear(embed_dim, 100), # FC1\n","            nn.ReLU(),\n","            nn.Linear(100, 1) # FC2\n","        )\n","\n","        self._init_weights(self.layers)\n","\n","    def _init_weights(self, m):\n","        \"\"\"Initialize network parameters \n","        \"\"\"\n","        initrange = 0.5\n","\n","        self.embedding.weight.data.uniform_(-initrange, initrange)\n","\n","        for i in range(len(m)):\n","          if type(m[i]) == nn.Linear:\n","            m[i].weight.data.uniform_(-initrange, initrange)\n","            m[i].bias.data.zero_()\n","\n","    def forward(self, text, offsets):\n","\n","      embedded = self.embedding(text)\n","\n","      output = []\n","\n","      # Embedding bag gives an output of 128 by 50, thus we need to make the outputs given by Embedding the same as in Embedding bag. \n","      # To do so, we need to make use of the offsets used by Embedding bag to manually get the mean of the columns for a group of rows \n","      # \n","      # If Embedded gives: \n","      #  [[1,2],\n","      #   [3,4],\n","      #   [5,6],\n","      #   [7,8]] \n","      # and offsets are [0,2], this means that the mean of columns should be calculated for row 0 and row 1, and row 2 and row 3\n","      #\n","      # Therefore the resulting mean tensor should be:\n","      #  [[(1+3)/2,(2+4)/2],\n","      #   [(5+7)/2,(6+8)/2]]\n","\n","      for i in range(len(offsets)):\n","\n","        # If not last offset, then is from offset to the next offset\n","        if i != len(offsets)-1:\n","          row_offset = embedded[offsets[i]:offsets[i+1], :]\n","\n","        # This is last element, thus take from last offset to end of embedded text list\n","        else:\n","          row_offset = embedded[offsets[i]:, :]\n","\n","        row_mean = torch.mean(row_offset, dim=0)\n","        output.append(row_mean) # Append each tensor into list\n","\n","      final_output = torch.stack(output, dim=0) # Convert list to a tensor with torch.stack()\n"," \n","      # Question 4: Use the new model you define in __init__()\n","      return torch.sigmoid(self.layers(final_output))"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gZS_06T3ja5a"},"source":["## Define train and test function"]},{"cell_type":"code","metadata":{"id":"yOR_liJ0ja5b","executionInfo":{"status":"ok","timestamp":1635007832765,"user_tz":-480,"elapsed":291,"user":{"displayName":"Dylan Tan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12191622771912211182"}}},"source":["import time\n","\n","def train(dataloader):\n","    model.train()\n","    total_acc, total_count = 0, 0\n","    log_interval = 50\n","    start_time = time.time()\n","\n","    for idx, (label, text, offsets) in enumerate(dataloader):\n","        optimizer.zero_grad()\n","        # forward propagation\n","        predicted_label = model(text, offsets)\n","        # calculate loss and backpropagate to model paramters\n","        loss = criterion(predicted_label, label)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n","        # update parameters by stepping the optimizer\n","        optimizer.step()\n","        total_acc += ((predicted_label > 0.5) == label).sum().item()\n","        total_count += label.size(0)\n","        if idx % log_interval == 0 and idx > 0:\n","            elapsed = time.time() - start_time\n","            print('| epoch {:3d} | {:5d}/{:5d} batches '\n","                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n","                                              total_acc/total_count))\n","            total_acc, total_count = 0, 0\n","            start_time = time.time()\n","\n","def evaluate(dataloader):\n","    model.eval()\n","    total_acc, total_count = 0, 0\n","\n","    with torch.no_grad():\n","        for idx, (label, text, offsets) in enumerate(dataloader):\n","            predicted_label = model(text, offsets)\n","            loss = criterion(predicted_label, label)\n","            total_acc += (predicted_label.argmax(1) == label).sum().item()\n","            total_count += label.size(0)\n","    return total_acc/total_count"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bTOKoHk-ja5e"},"source":["## Train and evaluate the model for several epochs"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Skb2CnFEja5e","executionInfo":{"status":"ok","timestamp":1635008705690,"user_tz":-480,"elapsed":802497,"user":{"displayName":"Dylan Tan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12191622771912211182"}},"outputId":"638ad5b6-f557-48ab-d7f2-b84980f1b86d"},"source":["# Hyperparameters\n","EPOCHS = 30 # epoch\n","patience = 10\n","highest_val_acc = 0\n","\n","model = LogisticRegression(word_vec=word_emb, embed_dim=EMBEDDING_DIM).to(device)\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n","criterion = torch.nn.BCELoss()\n","total_accu = None\n","\n","for epoch in range(1, EPOCHS + 1):\n","    epoch_start_time = time.time()\n","    train(train_dataloader)\n","    \n","    # Question 5: Use your validation set to early stop the model. Remember to early stop when the validation accuracy does not improve for continous N number of epochs where N is a hyperparameter. Set N = 10\n","    val_acc = evaluate(val_dataloader)\n","    print('------ Validation accuracy {:8.3f}% ------\\n'.format(val_acc))\n","\n","    if (val_acc >= highest_val_acc):\n","      highest_val_acc = val_acc\n","      patience = 10\n","    \n","    else:\n","      patience -= 1\n","\n","    if patience == 0:\n","      print(\"Early stopped to prevent overfitting...\")\n","      break\n"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["| epoch   1 |    50/  186 batches | accuracy    0.506\n","| epoch   1 |   100/  186 batches | accuracy    0.511\n","| epoch   1 |   150/  186 batches | accuracy    0.508\n","------ Validation accuracy   59.830% ------\n","\n","| epoch   2 |    50/  186 batches | accuracy    0.509\n","| epoch   2 |   100/  186 batches | accuracy    0.499\n","| epoch   2 |   150/  186 batches | accuracy    0.507\n","------ Validation accuracy   59.974% ------\n","\n","| epoch   3 |    50/  186 batches | accuracy    0.504\n","| epoch   3 |   100/  186 batches | accuracy    0.517\n","| epoch   3 |   150/  186 batches | accuracy    0.517\n","------ Validation accuracy   59.806% ------\n","\n","| epoch   4 |    50/  186 batches | accuracy    0.515\n","| epoch   4 |   100/  186 batches | accuracy    0.517\n","| epoch   4 |   150/  186 batches | accuracy    0.514\n","------ Validation accuracy   59.782% ------\n","\n","| epoch   5 |    50/  186 batches | accuracy    0.514\n","| epoch   5 |   100/  186 batches | accuracy    0.517\n","| epoch   5 |   150/  186 batches | accuracy    0.526\n","------ Validation accuracy   60.190% ------\n","\n","| epoch   6 |    50/  186 batches | accuracy    0.514\n","| epoch   6 |   100/  186 batches | accuracy    0.525\n","| epoch   6 |   150/  186 batches | accuracy    0.522\n","------ Validation accuracy   59.758% ------\n","\n","| epoch   7 |    50/  186 batches | accuracy    0.517\n","| epoch   7 |   100/  186 batches | accuracy    0.535\n","| epoch   7 |   150/  186 batches | accuracy    0.520\n","------ Validation accuracy   59.878% ------\n","\n","| epoch   8 |    50/  186 batches | accuracy    0.525\n","| epoch   8 |   100/  186 batches | accuracy    0.523\n","| epoch   8 |   150/  186 batches | accuracy    0.523\n","------ Validation accuracy   59.926% ------\n","\n","| epoch   9 |    50/  186 batches | accuracy    0.524\n","| epoch   9 |   100/  186 batches | accuracy    0.529\n","| epoch   9 |   150/  186 batches | accuracy    0.521\n","------ Validation accuracy   59.686% ------\n","\n","| epoch  10 |    50/  186 batches | accuracy    0.508\n","| epoch  10 |   100/  186 batches | accuracy    0.538\n","| epoch  10 |   150/  186 batches | accuracy    0.535\n","------ Validation accuracy   59.902% ------\n","\n","| epoch  11 |    50/  186 batches | accuracy    0.526\n","| epoch  11 |   100/  186 batches | accuracy    0.531\n","| epoch  11 |   150/  186 batches | accuracy    0.516\n","------ Validation accuracy   60.070% ------\n","\n","| epoch  12 |    50/  186 batches | accuracy    0.523\n","| epoch  12 |   100/  186 batches | accuracy    0.524\n","| epoch  12 |   150/  186 batches | accuracy    0.531\n","------ Validation accuracy   59.806% ------\n","\n","| epoch  13 |    50/  186 batches | accuracy    0.532\n","| epoch  13 |   100/  186 batches | accuracy    0.522\n","| epoch  13 |   150/  186 batches | accuracy    0.522\n","------ Validation accuracy   59.830% ------\n","\n","| epoch  14 |    50/  186 batches | accuracy    0.537\n","| epoch  14 |   100/  186 batches | accuracy    0.540\n","| epoch  14 |   150/  186 batches | accuracy    0.525\n","------ Validation accuracy   59.902% ------\n","\n","| epoch  15 |    50/  186 batches | accuracy    0.520\n","| epoch  15 |   100/  186 batches | accuracy    0.533\n","| epoch  15 |   150/  186 batches | accuracy    0.537\n","------ Validation accuracy   59.806% ------\n","\n","Early stopped to prevent overfitting...\n"]}]},{"cell_type":"code","metadata":{"id":"8eOGMmV6ja5f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635008780681,"user_tz":-480,"elapsed":45643,"user":{"displayName":"Dylan Tan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12191622771912211182"}},"outputId":"5e4b692b-332f-4f30-fd91-1a995a54edd0"},"source":["accu_test = evaluate(test_dataloader)\n","print('test accuracy {:8.2f}%'.format(accu_test))"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["test accuracy    63.91%\n"]}]}]}
